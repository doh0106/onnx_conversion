{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 629/629 [00:00<00:00, 140kB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 268M/268M [00:02<00:00, 92.2MB/s] \n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 48.0/48.0 [00:00<00:00, 17.3kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 588kB/s]\n"
     ]
    }
   ],
   "source": [
    "model_path = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_model_input = tokenizer(\"This is a sample\", return_tensors=\"pt\")\n",
    "print(dummy_model_input.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/models/distilbert/modeling_distilbert.py:223: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  mask, torch.tensor(torch.finfo(scores.dtype).min)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "# 변경할 onnx모델의 형식 지정\n",
    "torch.onnx.export(\n",
    "    model,\t\t\t\t\t\t\t            # model 위치\n",
    "    tuple(dummy_model_input.values()),\t\t\t# input\n",
    "    f=\"/root/onnx/model.onnx\",\t\t            # onnx모델 저장 경로\n",
    "    input_names=['input_ids', 'attention_mask'],\n",
    "    output_names=[\"logits\"],\n",
    "    dynamic_axes={\n",
    "        \"input_ids\":{0: \"batch_size\", 1: \"sequence\"},\n",
    "        \"attention_mask\":{0: \"batch_size\", 1: \"sequence\"},\n",
    "        \"logits\":{0:'batch_size'},\n",
    "                  },\n",
    "    do_constant_folding=True, \n",
    "    opset_version=13,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx \n",
    "import onnxruntime as ort \n",
    "\n",
    "onnx_path = \"/root/onnx/model.onnx\"\n",
    "onnx_model = onnx.load(onnx_path)\n",
    "\n",
    "sess = ort.InferenceSession(onnx_path)\n",
    "\n",
    "# model_optimum = onnx.load(\"/root/onnx/optimum.onnx/model.onnx\")\n",
    "sess_optimum = ort.InferenceSession(\"/root/onnx/optimum.onnx/model.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_input = {}\n",
    "for input in sess.get_inputs():\n",
    "    input_name = input.name\n",
    "    input_data = dummy_model_input[input_name].numpy()\n",
    "    onnx_input[input_name]=input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimum_output = sess_optimum.run(None, onnx_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.5395,  2.6369]], device='cuda:0'), hidden_states=None, attentions=None)\n",
      "[array([[-2.5394607,  2.6368706]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "onnx_output = sess.run(None, onnx_input)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.to('cuda') \n",
    "    outputs = model(**dummy_model_input.to('cuda'))\n",
    "    model.to('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "np.allclose(outputs.logits.cpu(), onnx_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(outputs.logits.cpu(), optimum_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
